---
recall: header
---

### Welche zwei Modi zur Verwendung von Spark gibt es?

- interaktiv (gut zum Lernen)
- batch (Produktivnutzung)

### Was ist die gängigste Struktur in Spark zur Verwaltung strukturierter Daten?

DataFrame

### Was ist ein *Schema* in Spark?

die Beschreibung eines DataFrame: Spaltennamen und Datentypen

### Was ist eine *Partition* in Spark?

ein Teil eines RDD / DataFrames, der auf einem Knoten des Clusters gespeichert ist

### Was ist eine *Transformation* in Spark?

eine Anweisung, wie aus einem RDD / DataFrame ein neues RDD / DataFrame berechnet werden soll

### Welche zwei Arten von Transformationen gibt es in Spark?

- enge Abhängigkeiten (narrow transformations)
- breite Abhängigkeiten (wide transformations, shuffle)

### Was ist eine *narrow transformation* in Spark?

eine Transformation, bei der eine Eingabepartition zu einer Ausgabepartition führt, also eine
Berechnung die nur von einem Wert abhängig ist und zu einem anderen Wert führt

### Was ist eine *wide transformation* in Spark?

eine Transformation, bei der eine Eingabepartition zu mehreren Ausgabepartitionen führen kann

### Was sind Beispiele für *wide transformations* in Spark?

- Sortieren
- Gruppieren (?)
- TODO

### Was bedeutet *lazy evaluation* in Spark?

Transformationen werden nicht sofort angewendet, sondern nur als Berechnungsanweisung gespeichert.
Erst wenn eine Aktion ausgeführt wird.

### Welche Vorteile bietet *lazy evaluation* in Spark?

Berechnungsfolgen lassen sich vor der Ausführung optimieren und bestmöglich auf die Clusterknoten
verteilen

### Was ist eine *Aktion* in Spark?

eine Operation, um eine Reihe von Transformationen zu triggern und das Ergebnis der Berechnungen zu
erhalten, das kein RDD ist

### Was ist der Unterschied zwischen Transformation und Aktion in Spark?

Transformationen erzeugen neue RDDs auf Basis von einem oder mehreren RDDs. Sie werden erst
ausgeführt wenn es unbedingt nötig wird (lazy evaluation). Aktionen triggern die Berechnung von
Transformationen und Berechnen ein Ergebnis, was kein RDD ist.

### Welche drei Arten von Aktionen gibt es in Spark?

Aktionen
1. zur Anzeige von Daten
2. zum Sammeln von Daten
3. zum Schreiben von Ausgabedateien

### Wie viele Zeilen eines DataFrame zeigt der `show()` Befehl in Spark wenn er ohne Parameter aufgerufen wird?

für gewöhnlich 20

### Mit welchem Kommandozeilenbefehl werden Spark Batch Jobs abgesetzt?

`spark-submit`

### Was ist der Unterschied von DataSets und DataFrames in Spark?

- DataSets sind statisch typisiert, DataFrames dynamisch
- DataSets nur in Scala und Java verfügbar
- DataFrames sind DataSets mit Datentyp *row*

### Wozu dient die Komponente *Catalyst* in Spark?

Optimierung der Ausführung von Transformationen

### Mit welchen Typen Arbeitet Spark?

Spark hat ein eigenes Typsystem, die teilweise ein 1:1 Mapping zu den Datentypen der verwendeten
Sprache besitzen

### Was ist ein *Plan* in Spark?

TODO

### Welche Folge von Transformationen durchläuft Benutzercode in Spark vor der Ausführung?

Anwendercode $\rightarrow$ nicht aufgelöster logischer Plan $\rightarrow$ aufgelöster logischer Plan
$\rightarrow$ optimierter Logischer Plan

### Worauf bezieht sich "nicht aufgelöst" in einem nicht aufgelösten logischer Plan von Spark?

die Existenz der Tabellen und Spalten, die der Plan verwendet, ist noch nicht geprüft

### Was ist die Aufgabe des *Analyzer* in Spark?

Umwandlung eines nicht aufgelösten logischer Plans in einen aufgelösten logischer Plan durch
Überprüfen der Validität der verwendeten Tabellen und Spalten auf Basis eines "Katalogs" aller
Tabellen- und DataFrame-Informationen

### Warum wird Spark auch als Compiler bezeichnet?

weil es Abfragen in DataFrames, DataSets und SQL empfängt und daran RDD-Transformationen entwickelt

### Welche zwei APIs bietet Spark?

Structured API: Verwendung von DataSets, DataFrames und SQL Low-Level API: Verwendung von RDDs und
Distributed Variables

### Welche Optimierungen führt Spark auf einem Plan aus?

- Optimierung des logischen Plans durch Catalyst
- Auswahl des besten physischen Plans auf Basis eines Kostenmodells
- Optimierung des Java Bytecode des physischen Plans

### Was ist eine *Stufe* (Stage) in Spark?

eine Gruppe von Tasks, die zusammen ausgeführt werden können, um dieselbe Operation auf mehreren
Rechnern gleichzeitig zu berechnen

### Was veranlasst Spark zum Starten einer neuen Stufe?

neue Stufen werden nach Shuffle-Operationen (also physischer Neuverteilung der Daten) gestartet

### Was ist ein Task in Spark?

Kombination aus Partition und (ggf. mehrere) Transformation(en), die kleinste Aufgabeneinheit, die
durch einen Executor auf einem Knoten ausgeführt wird

### Wozu dient das Pipelining in Spark?

Aneinanderreihung von möglichst vielen Berechnungsschritten bevor Daten auf die Festplatte
zurückgeschrieben werden, um die Gesamtrechenzeit zu reduzieren

### Wie wird eine DataFrameReader in Spark erstellt?

`spark.read.format("csv"/"json"/...)` erstellt einen Reader und mit der ` .option()` Methode werden
Leseoptionen festgelegt

### Welche drei Lesemodi für fehlerhafte Datensätze hat der DataFrameReader von Spark?

1. PERMISSIVE: defekte Records (Zeilen der Tabelle) werden auf null gesetzt und der Rohdatenstring
   in eine String Spalte abgelegt (Name kann vorgegeben werden)
2. DROPMALFORMED: defekte Records werden einfach gelöscht / ignoriert
3. FAILFAST: defekte Records lösen eine Exception aus

### Mit welchem Befehl kann die lineage eines DataFrames in PySpark angezeigt werden?

`df.explain()`

