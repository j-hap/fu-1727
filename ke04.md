---
recall: header
---

### Wofür steht die Abkürzung MPI?

Message Passing Interface

### Was ist das Message Passing Interface?

eine Spezifikation, die den de-factor Standard für paralleles Programmieren mit Message Passing
beschreibt

### Was ist der Unterschied zwischen OpenMP und MPI?

- Thread-parallel vs. prozess-parallel
- gemeinsamer Speicher vs. verteilter / getrennter Speicher

### Warum kann bei MPI nicht über gemeinsam genutzte Datenstrukturen kommuniziert werden?

bei MPI werden mehrere Prozesse mit unterschiedlichen Adressräumen gestartet, die entsprechend nicht
auf den gleichen Speicherbereich zugreifen können, was nötig wäre um eine Datenstruktur gemeinsam zu
nutzen

### Warum werden bei Prozess-paralleler Berechnug die Eingabedaten nicht einfach auf alle Knoten kopiert?

für gewöhnlich sind die Eingabedaten so groß, dass nicht jeder Knoten eine Kopie davon im RAM halten
kann

### Wie werden bei Prozess-paralleler Berechnung die Eingabedaten auf die Knoten gebracht?

Gesamtmenge von Eingabedaten wird aufgeteilt, sodass jeder Knoten die für ihn wichtigen Daten hat

### Wozu benötigt man bei Prozess-paralleler Berechnung einen Nachrichtenaustausch?

für gewöhnlich sind die Ergebnisse der einzelnen Knoten teilweise von den Ergebnissen der
Nachbarknoten abhängig, die in regelmäßigen Abständen kommuniziert werden müssen

### Was sind Beispiele für die Implementierung des MPI-Standards

- OpenMPI
- MPICH

### Wie wird ein Prozess-paralleles Programm gestartet?

über Wrapper-Executables des MPI Frameworks (`mpirun` oder `mpiexec`), dem die gewünschte Anzahl an
Prozessen und das auszuführende Programm übergeben wird

### Was ist die Aufgabe der MPI-Laufzeitumgebung?

- Starten von parallelen Prozessen auf der verwendeten Hardware (kann ein Rechner, können mehrere
  sein)
- TODO

### Welche Aufrufe müssen in ein MPI Programm eingebaut werden, um MPI-Befehle benutzen zu können?

- zuerst: `MPI_Init`
- zuletzt: `MPI_Finalize`

### Was ist die Aufgabe von `MPI_Init`?

- Kommunikationsstrukturen aufbauen
- Kommunikations-Buffer und Meta-Strukturen allokieren

### Was ist ein MPI Kommunikator?

Kommunikatoren beschreiben Gruppen von Prozessen, die untereinander kommunizieren und über die die
Kommunikation abläuft

### Wie kann ein Prozess seinen eigenen Rang innerhalb eines MPI Kommunikators abfragen?

mit der Funktion `MPI_Comm_rank`

### Wie kann ein Prozess die Anzahl Prozesse in einem MPI Kommunikator abfragen?

mit der Funktion `MPI_Comm_size`

### Welche MPI Kommunikatoren sind standardmäßig nach der Initialisierung vorhanden?

- MPI_COMM_WORLD (alle Prozesse)
- MPI_COMM_SELF (nur der Prozess selbst)

### Warum werden MPI Kommunikatoren verwendet?

- kleinere Gruppen von Prozessen zusammenfassen (gleicher Rechner / gleiches Teilproblem)
- *separation of concerns* - einen eigenen Kommunikationsraum zum Beispiel für die Prozesse einer
  Bibliothek schaffen

### Wie kompiliert man MPI Programme?

mit Wrappern wie `mpicc` oder `mpic++`

### Mit welchem MPI Befehl kann der Rang des aktuellen Prozesses abgefragt werden?

`MPI_Comm_rank`

### Welcher Befehl dient bei MPI zum Senden von Nachrichten über einen Kommunikator?

`MPI_Send`

### Welche Parameter erhält der `MPI_Send` Befehl?

- Pointer zum Buffer mit Daten
- Anzahl gesendete Elemente
- Datentyp
- Prozess-Rang des Empfängers
- Tag (ggf. für Filterung von Nachrichten)
- Kommunikator

### Welcher Befehl dient bei MPI zum Empfangen von Nachrichten über einen Kommunikator?

MPI_Recv

### Welche Parameter erhält der `MPI_Recv` Befehl?

- Pointer zum Buffer in dem die Daten landen sollen
- Anzahl zu empfangende Elemente
- Datentyp
- Prozess-Rang des Senders
- Tag (ggf. für Filterung von Nachrichten)
- Kommunikator
- Pointer auf Statusvariable

### Mit welchem Rang kann der `MPI_Recv` Daten aller Sender empfangen?

MPI_ANY_SOURCE (Wildcard)

### Mit welchem Rang kann der `MPI_Recv` Daten mit jedem Tag empfangen?

MPI_ANY_TAG (Wildcard)

### Was kann über die Status-Variable beim Empfang einer MPI Nachricht abgeprüft werden?

1. Anzahl der empfangenen Elemente
2. Rang des Senders
3. Tag der Nachricht
4. Mögliche Fehler

### Welcher Fehler in der MPI Statusvariable signalisiert, dass eine empfange Nachricht zu lang war?

`MPI_ERR_TRUNCATED`

### Wie kann beim Empfangen einer Nachricht der Status aus Performancegründen ausgelassen werden?

`MPI_STATUS_IGNORE` als Pointer zur Statusvariable übergeben

### Was ist ein *Deadlock*?

ein Konflikt im Ablauf eines Programms, bei dem mehrere Prozesse / Threads aufeinander warten und
der Programmablauf zum erliegen kommt

### Was ist das besondere an gepuffertem Senden bei MPI?

Daten werden vom sendenden Prozess in einem lokalen MPI Puffer geladen, sodass der Sendevorgang für
den sendenden Prozess als abgeschlossen gilt

### Wie kann man bei MPI gepuffertes Senden erzwingen?

`MPI_Bsend`

### Wie kann der Entwickler den verwendeten Buffer für gepuffertes Senden festlegen?

`MPI_Buffer_attach`

### Wie kann der Entwickler das Versenden gepufferter Nachrichten erzwingen?

`MPI_Buffer_detach` nachdem vorher mit `MPI_Buffer_attach` der Puffer festgelegt wurde

### Welche Informationen beinhaltet die Konstante `MPI_BSEND_OVERHEAD`?

wie viele Bytes MPI selbst bei gepuffertem Senden neben den Daten mit in den Buffer legt

### Welche zwei Protokolle gibt es beim Senden von Nachrichten mit MPI?

- Eager (etwa wie UPD, wird einfach raus geschickt, ohne vorab Bereitschaft des Empfängers zu
  erfragen)
- Rendezvouz (etwa wie TCP, Senden wird erst beendet, wenn Empfänger empfangen hat) 

### Wie kann man bei MPI synchronisiertes Senden erzwingen?

`MPI_Ssend`

### Welche Nachteile hat gepuffertes Senden?

- Speicheroverhead
- ggf. unnötiges Kopieren von Daten

### In welche Arten der Kommunikation werden in MPI unterschieden?

- synchrone (blocking)
- asynchrone (non-blocking)

### Welche drei Befehle dienen zur Steuerung von asynchronem Nachrichtenaustausch in MPI?

- `MPI_Isend`
- `MPI_Irecv`
- `MPI_Wait`

### Welche Reihenfolge von asynchroner Kommunikation (senden / empfangen) ist günstiger in Bezug auf Performance?

erst alle Empfangsvorgänge initialisieren, dann die Sendevorgängen und dann auch erst auf die
Empfangsvorgänge warten und dann auf die Sendevorgänge

### Welche Befehle stehen in MPI zur Verfügung um auf mehrere asynchrone Kommunikationen gleichzeitig zu warten?

- `MPI_Waitall`
- `MPI_Waitaany`
- `MPI_Waitsome`

### Wann gibt `MPI_Waitall` die Kontrolle zurück?

wenn alle übergebenen `MPI_Request`s fertig sind

### Wann gibt `MPI_Waitany` die Kontrolle zurück?

wenn einer der übergebenenen `MPI_Request`s fertig ist

### Welchen Index gibt `MPI_Waitany` wenn mehr als ein Request gleichzeitig fertig wird?

einen zufälligen

### Welchen Vorteil hat `MPI_Waitsome` gegenüber `MPI_Waitany`?

es werden die Indices alles fertiggestellen Requests zurückgegeben und nicht nur eines (falls
mehrere gleichzeitig fertig werden)

### Was ist der Unterschied zwischen Wait und Test Befehlen in MPI?

Wait blockiert die Befehlsausführung bis eine (any / some) bzw. alle Kommunikationen (all) beendet
wurden, Test hingegen überprüft nur, ob eine Kommunikation beendet wurde und gibt `false`zurück wenn
dem nicht so ist

### Mit welchem Befehl kann abgefragt werden, ob ein asynchroner Kommunikations-Request schon abgeschlossen ist?

`MPI_Test`

### Welche Befehle stehen in MPI zur Verfügung um den Status mehrerer asynchroner Kommunikationen gleichzeitig abzufragen?

- `MPI_Testall`
- `MPI_Testaany`
- `MPI_Testsome`

### Was sind Kollektive Operationen in MPI?

Befehle, die nicht nur zwischen zwei Prozessen kommunizieren, sondern zwischen allen eines
Kommunikators

### Welchen Vorteil haben Kollektive Operationen in MPI gegenüber Send- und Recv-Befehlen?

- optimiert auf verwendete Architektur
- optimiert auf Datengröße

### Wozu dient die Kollektive Operation `MPI_Barrier`?

alle Prozesse eines Kommunikators müssen die Barriere erreichen bevor der erste Prozess sie
überschreiten und weiterrechen darf, dient der Synchronisation der Prozesse

### Mit welchem MPI Befehl lässt sich eine Nachricht an alle Prozesse in einem Kommunikator schicken?

`MPI_Bcast`

### Mit welchem MPI-Befehl lässt sich ein Array in $n$ gleichgroßen Stücken an $n$ Prozessen aufteilen?

`MPI_Scatter`

### An welche Prozesse werden beim `MPI_Scatter`-Befehl von wem die Daten gesendet?

von einem Wurzelprozess an alle Prozesse in einem Kommunikator, inklusive dem Wurzelprozess

### Mit welchem MPI-Befehl lassen sich Daten-Stücke von allen Prozessen in einem Kommunikator einsammeln?

`MPI_Gather`

### Welcher MPI Befehl gleicht dem reduce Pragma von OpenMP?

`MPI_Reduce`

### Mit welchem Befehl lassen sich Ergebnisse von allen Prozessen eines Kommunikators sammeln und miteinander verrechnen?

`MPI_Reduce`

### Welche Bedingung wird an die Reduzierungs-Operation des `MPI_Reduce`-Befehls gestellt?

muss kommutativ und assoziativ sein

### Was ist der Unterschied zwischen `MPI_Reduce` und `MPI_Allreduce`?

bei `MPI_Reduce` hat am Ende nur der als root angegebene Prozess das Ergebnis, bei `MPI_Allreduce`
wird das Ergebnis wieder an alle Prozesse verteilt

### Mit welchem Befehl erstellt man einen Kommunikator auf Basis eines existierenden z.B. zur Verwendung in einer eigenen Bibliothek?

`MPI_Comm_dup`

### Mit welchem Befehl erstellt man einen Kommunikator auf Basis eines existierenden zur Verwendung in einer eigenen Bibliothek?

`MPI_Comm_split`

### Mit welchem Befehl wird ein nicht mehr benötigter Kommunikator gelöscht?

`MPI_Comm_free`

### Was ist das Controller-Worker-Pattern?

eine Methode der dynamischen Lastverteilung zwischen Prozessen in MPI. Controller verteilt die Last
auf die Worker